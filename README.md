<img width="1918" height="1082" alt="image" src="https://github.com/user-attachments/assets/3c1bd2d0-1e7e-43b9-81ec-77aa83faf9db" />

This Hand Gesture Recognition model combines Convulation Neural Networks with Mediapipe hand landmarks.
We have 9 gestures at present - open palm, ok, left,right,up,down,index,fist, and one more...

## Project Structure
Hand Gesture/
â”‚
â”œâ”€â”€ .venv/                     # Virtual environment
â”‚
â”œâ”€â”€ dataset/                   # Dataset folder (contains gesture images)
â”‚
â”œâ”€â”€ .gitattributes
â”œâ”€â”€ .gitignore
â”‚
â”œâ”€â”€ dataset_creation.ipynb     # Notebook for dataset preparation
â”œâ”€â”€ model.ipynb                # Model training and evaluation
â”œâ”€â”€ livecontrol.py             # Real-time gesture detection and control script
â”‚
â”œâ”€â”€ hand-gesture-model.keras   # Trained CNN + Landmark fusion model
â”œâ”€â”€ hand_gestures.csv          # Gesture labels / metadata
â”œâ”€â”€ le.pkl                     # Label encoder for gesture classes
â”œâ”€â”€ scaler.pkl                 # Feature scaler for numerical normalization
â”‚
â””â”€â”€ README.md                  # Project documentation (this file)

## ðŸ§  Model Overview

The model uses a **hybrid deep learning approach**:
- **CNN** extracts **visual features** from raw gesture images.
- **MediaPipe** extracts **21-hand landmark coordinates** per frame.
- These two feature vectors are **combined** and passed through dense layers for classification.

### ðŸ§© CNN + ANN Architecture
```python
model = Sequential([
    Input(shape=input_shape),
    Conv2D(32, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Conv2D(128, (3,3), activation='relu'),
    MaxPooling2D(pool_size=(2,2)),
    Flatten(),
])
y_out = Dense(128, activation='relu')(combined_input)
y_out = Dropout(0.3)(y_out)
y_out = Dense(64, activation='relu')(y_out)
y_out = Dense(num_classes, activation='softmax')(y_out) 
```
## Compilation & Training
Optimizer: Adam(learning_rate=0.001)
Batch size: 1
Epochs: 17
Loss function: Categorical Crossentropy
Metrics: Accuracy

## workflow
Dataset Creation:
Run dataset_creation.ipynb to capture and label gesture images using your webcam and MediaPipe.
Model Training:
Use model.ipynb to train the CNN and fusion network on the prepared dataset.
Real-Time Detection:
Run livecontrol.py to use your webcam for live gesture prediction and real-time application control.

